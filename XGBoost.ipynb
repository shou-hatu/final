{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各ライブラリのimport\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0805420080889037"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost\n",
    "#読み込みから標準化まで(trainのみ)\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('データ/train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train.iloc[24 , 4] = df_train[df_train['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train.iloc[113 , 4] = df_train[(98 <= df_train['displacement']) & (df_train['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train.iloc[145 , 4] = df_train[df_train['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train.iloc[175 , 4] = df_train[df_train['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "#pwrカラム作成\n",
    "#df_train['pwr'] = df_train['weight']/df_train['horsepower']\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train['manufacturers name'] = df_train['manufacturers name'].replace({\"toyouta\":\"toyota\", \"vw\":\"volkswagen\", \"vokswagen\":\"volkswagen\",  \"chevroelt\":\"chevrolet\" ,  \"chevy\":\"chevrolet\",\"mercury\":\"ford\", \"datsun\":\"nissan\", \"maxda\":\"mazda\",  \"mercedes\":\"mercedes-benz\"})#カラム名の修正\n",
    "#エンコーディング\n",
    "df_number = pd.get_dummies(df_train , columns = ['origin' , 'manufacturers name'] , dtype=int)\n",
    "#特徴量の分割\n",
    "features = df_number.drop(['id' , 'cylinders' ,'weight' , 'car name' , 'mpg'] , axis = 1)\n",
    "#print(features)\n",
    "target = df_train['mpg']\n",
    "#データ分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train , features_test , target_train , target_test = train_test_split(features , target , test_size = 0.25 , random_state = 0)\n",
    "#学習\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor(n_estimators = 100)\n",
    "model.fit(features_train , target_train)\n",
    "pred = model.predict(features_test)\n",
    "#評価\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "sqrt(mean_squared_error(target_test, pred)) \n",
    "#pwrカラムなし,horsepowerあり,accelerationあり;2.92\n",
    "#pwrカラムなし,horsepowerなしaccelerationあり;3.098\n",
    "#pwrカラムあり,horsepowerなし,accelerationあり;2.996\n",
    "#pwrカラムあり,horsepowerなし,accelerationなし;3.390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6592625087884603"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost\n",
    "#gridsearch\n",
    "#読み込みから標準化まで(trainのみ)\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('データ/train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train.iloc[24 , 4] = df_train[df_train['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train.iloc[113 , 4] = df_train[(98 <= df_train['displacement']) & (df_train['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train.iloc[145 , 4] = df_train[df_train['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train.iloc[175 , 4] = df_train[df_train['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "#pwrカラム作成\n",
    "df_train['pwr'] = df_train['weight']/df_train['horsepower']\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train['manufacturers name'] = df_train['manufacturers name'].replace({\"toyouta\":\"toyota\", \"vw\":\"volkswagen\", \"vokswagen\":\"volkswagen\",  \"chevroelt\":\"chevrolet\" ,  \"chevy\":\"chevrolet\",\"mercury\":\"ford\", \"datsun\":\"nissan\", \"maxda\":\"mazda\",  \"mercedes\":\"mercedes-benz\"})#カラム名の修正\n",
    "#エンコーディング\n",
    "df_number = pd.get_dummies(df_train , columns = ['origin' , 'manufacturers name'] , dtype=int)\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_train_drop = df_number.drop(['mpg','id' , 'car name' , 'weight' , 'cylinders' , 'horsepower'] , axis = 1)\n",
    "df_train_std = pd.DataFrame(scaler.fit_transform(df_train_drop), columns = df_train_drop.columns)\n",
    "\n",
    "\n",
    "#特徴量の分割\n",
    "features = df_train_std\n",
    "target = df_train['mpg']\n",
    "#データ分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train , features_test , target_train , target_test = train_test_split(features , target , test_size = 0.25 , random_state = 0)\n",
    "#学習\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor()\n",
    "#gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 最終的なパラメータ範囲\n",
    "cv_params = {'n_estimators': [20,40,60,80,100],\n",
    "             'learning_rate': [0.01, 0.03, 0.1, 0.3],#学習率(0～1)\n",
    "             'min_child_weight': [2, 4, 6, 8],#決定木の葉の重みの下限\n",
    "             'max_depth': [1, 2, 3, 4],#決定木の最大深度(整数)\n",
    "             'colsample_bytree': [0.2, 0.5, 0.8, 1.0],#説明変数のサンプル抽出比(木)(0~1)\n",
    "             'subsample': [0.2, 0.5, 0.8, 1.0]#各決定木のサンプル抽出比(0~1)、小さいほど保守的になる\n",
    "             }\n",
    "# グリッドサーチのインスタンス作成\n",
    "#引数一覧\n",
    "#estimator: チューニングを行うモデル\n",
    "#param_grid: パラメータ候補パラメータ名: [候補リスト]\n",
    "#scoring: 評価指標(今回はneg_mean_squared_error(RSME))\n",
    "#cv: Cross Validationの分割数(default: 3)\n",
    "#verbose: ログ出力レベル\n",
    "#n_jobs: 同時実行数(-1: コア数で並列実行)\n",
    "#refit: 　trueのとき最良のパラメータで再学習\n",
    "grid_model = GridSearchCV(model, cv_params,cv = 5,\n",
    "                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_model.fit(features_train , target_train)\n",
    "pred = grid_model.predict(features_test)\n",
    "#評価\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "sqrt(mean_squared_error(target_test, pred)) \n",
    "#pwrカラムなし,horsepowerあり,accelerationあり;2.62\n",
    "#pwrカラムなし,horsepowerなしaccelerationあり;2.72\n",
    "#pwrカラムあり,horsepowerなし,accelerationあり;2.53\n",
    "#pwrカラムあり,horsepowerなし,accelerationなし;2.85\n",
    "#pwrカラムあり,horsepowerあり,accelerationなし;3.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル実装(train,test結合版)\n",
    "\n",
    "#gridsearchなし\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('データ/train.tsv' , sep = '\\t')\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_train_test = pd.concat([df_train , df_test] , axis =0)\n",
    "#print(df_train_test[df_train_test['horsepower'] == \"?\"])\n",
    "df_train_test['horsepower'] = df_train_test['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train_test.iloc[24 , 4] = df_train_test[df_train_test['displacement'] == 151]['horsepower'].mean()\n",
    "#renault 18\n",
    "df_train_test.iloc[113 , 4] = df_train_test[(98 <= df_train_test['displacement']) & (df_train_test['displacement']<= 102)]['horsepower'].mean()\n",
    "#renault lecar deluxe\n",
    "df_train_test.iloc[145 , 4] = df_train_test[df_train_test['displacement'] == 85]['horsepower'].mean()\n",
    "#ford pinto\n",
    "df_train_test.iloc[175 , 4] = df_train_test[df_train_test['car name'] == 'ford pinto']['horsepower'].mean()\n",
    "#ford maverick\n",
    "df_train_test.loc[70 , 'horsepower'] = df_train_test[df_train_test['car name'] == \"ford maverick\"]['horsepower'].mean()\n",
    "#ford mustang cobra\n",
    "df_train_test.loc[112 , 'horsepower'] = df_train_test[df_train_test['displacement'] == 140]['horsepower'].mean()\n",
    "#pwrカラム作成\n",
    "df_train_test['pwr'] = df_train_test['weight']/df_train_test['horsepower']\n",
    "#ラベルエンコーディング(train,test結合版)\n",
    "#メーカーカラム作成\n",
    "df_split = df_train_test['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train_test['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train_test['manufacturers name'] = df_train_test['manufacturers name'].replace({\"toyouta\":\"toyota\", \"vw\":\"volkswagen\", \"vokswagen\":\"volkswagen\",  \"chevroelt\":\"chevrolet\" ,  \"chevy\":\"chevrolet\",\"mercury\":\"ford\", \"datsun\":\"nissan\", \"maxda\":\"mazda\",  \"mercedes\":\"mercedes-benz\"})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "df_number = pd.get_dummies(df_train_test , columns = ['origin' , 'manufacturers name'] , dtype = int)\n",
    "#print(df_number)\n",
    "df_train = df_number.iloc[:198 , :]\n",
    "df_test = df_number.iloc[199: , :]\n",
    "\n",
    "#目的変数の分離\n",
    "features_train = df_train.drop(['weight', 'id' , 'car name' , 'mpg','horsepower' , 'cylinders','acceleration'] , axis = 1)\n",
    "target_train = df_train['mpg']\n",
    "\n",
    "features_test = df_test.drop(['weight' , 'id' , 'car name' , 'mpg','horsepower' , 'cylinders','acceleration'] , axis = 1)\n",
    "#学習\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor()\n",
    "model.fit(features_train , target_train)\n",
    "\n",
    "df_pred = pd.DataFrame(model.predict(features_test))\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_pred = pd.merge(df_test['id'].astype(int) , df_pred , how = 'outer' , right_index = True , left_index=True)\n",
    "df_pred\n",
    "#評価\n",
    "df_pred.to_csv('XGBsample_submit.csv' , header = False , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29.0\n",
      "1    31.9\n",
      "2    19.0\n",
      "3    28.0\n",
      "4    37.7\n",
      "Name: mpg, dtype: float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 90\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# グリッドサーチのインスタンス作成\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#引数一覧\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#estimator: チューニングを行うモデル\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#n_jobs: 同時実行数(-1: コア数で並列実行)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#refit: 　trueのとき最良のパラメータで再学習\u001b[39;00m\n\u001b[0;32m     88\u001b[0m grid_model \u001b[38;5;241m=\u001b[39m GridSearchCV(model, cv_params,cv \u001b[38;5;241m=\u001b[39m cv,\n\u001b[0;32m     89\u001b[0m                       scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[43mgrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m df_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(grid_model\u001b[38;5;241m.\u001b[39mpredict(features_test))\n\u001b[0;32m     93\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mデータ/test.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m , sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#モデル実装(train,test結合版)#実装途中\n",
    "\n",
    "\n",
    "#gridsearch版\n",
    "#kfoldを入れ、pwrカラムを追加しない時が一番高かった。\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('データ/train.tsv' , sep = '\\t')\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_train_test = pd.concat([df_train , df_test] , axis =0)\n",
    "#print(df_train_test[df_train_test['horsepower'] == \"?\"])\n",
    "df_train_test['horsepower'] = df_train_test['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train_test.iloc[24 , 4] = df_train_test[df_train_test['displacement'] == 151]['horsepower'].mean()\n",
    "#renault 18\n",
    "df_train_test.iloc[113 , 4] = df_train_test[(98 <= df_train_test['displacement']) & (df_train_test['displacement']<= 102)]['horsepower'].mean()\n",
    "#renault lecar deluxe\n",
    "df_train_test.iloc[145 , 4] = df_train_test[df_train_test['displacement'] == 85]['horsepower'].mean()\n",
    "#ford pinto\n",
    "df_train_test.iloc[175 , 4] = df_train_test[df_train_test['car name'] == 'ford pinto']['horsepower'].mean()\n",
    "#ford maverick\n",
    "df_train_test.loc[70 , 'horsepower'] = df_train_test[df_train_test['car name'] == \"ford maverick\"]['horsepower'].mean()\n",
    "#ford mustang cobra\n",
    "df_train_test.loc[112 , 'horsepower'] = df_train_test[df_train_test['displacement'] == 140]['horsepower'].mean()\n",
    "#pwrカラム作成in_test['displacement']\n",
    "#df_train_test['pwr'] = df_train_test['weight']/df_train_test['horsepower']\n",
    "#ラベルエンコーディング(train,test結合版)\n",
    "#メーカーカラム作成\n",
    "df_split = df_train_test['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train_test['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train_test['manufacturers name'] = df_train_test['manufacturers name'].replace({\"toyouta\":\"toyota\", \"vw\":\"volkswagen\", \"vokswagen\":\"volkswagen\",  \"chevroelt\":\"chevrolet\" ,  \"chevy\":\"chevrolet\",\"mercury\":\"ford\", \"datsun\":\"nissan\", \"maxda\":\"mazda\",  \"mercedes\":\"mercedes-benz\"})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "df_number = pd.get_dummies(df_train_test , columns = ['origin' , 'manufacturers name'] , dtype = int)\n",
    "df_train = df_number.iloc[:198 , :]\n",
    "df_test = df_number.iloc[199: , :]\n",
    "#学習\n",
    "#import\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "#データ変換\n",
    "features_train = xgb.DMatrix(df_train.drop([ 'id' , 'car name' , 'mpg'] , axis = 1) , )\n",
    "#目的変数の分離\n",
    "features_train = \n",
    "target_train = df_train['mpg']\n",
    "print(target_train.head())\n",
    "features_test = df_test.drop([  'id' , 'car name' , 'mpg'] , axis = 1)\n",
    "#評価用データ\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_train , train_eval , target_target , target_eval  = train_test_split(features_train , target_train , test_size=0.2 , random_state = 0)\n",
    "\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "                     random_state=42, n_estimators=10000 )\n",
    "fit_params = {'verbose': 0,  # 学習中のコマンドライン出力\n",
    "              'early_stopping_rounds': 10,  # 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\n",
    "              'eval_metric': 'rmse',  # early_stopping_roundsの評価指標\n",
    "              'eval_set': [(train_train, target_eval)]  # early_stopping_roundsの評価指標算出用データ\n",
    "              }\n",
    "# 学習時fitパラメータ指定\n",
    "#fit_params = {'verbose': 0,  # 学習中のコマンドライン出力\n",
    "              #'early_stopping_rounds': 10,  # 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\n",
    "              #'eval_metric': 'rmse',  # early_stopping_roundsの評価指標\n",
    "              #}\n",
    "#K-分割交差検証\n",
    "\n",
    "#交差検証は比較的少ないデータセットを使って学習する場合に、過学習を防ぐ（汎化性能を上げる）ためにする\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 最終的なパラメータ範囲\n",
    "cv_params = {'learning_rate': [0.01, 0.03, 0.1, 0.3],#学習率(0～1)\n",
    "             'min_child_weight': [2, 4, 6, 8],#決定木の葉の重みの下限\n",
    "             'max_depth': [1, 2, 3, 4],#決定木の最大深度(整数)\n",
    "             'colsample_bytree': [0.2, 0.5, 0.8, 1.0],#説明変数のサンプル抽出比(木)(0~1)\n",
    "             'subsample': [0.2, 0.5, 0.8, 1.0]#各決定木のサンプル抽出比(0~1)、小さいほど保守的になる\n",
    "             }\n",
    "# グリッドサーチのインスタンス作成\n",
    "#引数一覧\n",
    "#estimator: チューニングを行うモデル\n",
    "#param_grid: パラメータ候補パラメータ名: [候補リスト]\n",
    "#scoring: 評価指標(今回はneg_mean_squared_error(RSME))\n",
    "#cv: Cross Validationの分割数(default: 3)\n",
    "#verbose: ログ出力レベル\n",
    "#n_jobs: 同時実行数(-1: コア数で並列実行)\n",
    "#refit: 　trueのとき最良のパラメータで再学習\n",
    "\n",
    " \n",
    "grid_model = GridSearchCV(model, cv_params,cv = cv,\n",
    "                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_model.fit(features_train , target_train )\n",
    "\n",
    "df_pred = pd.DataFrame(grid_model.predict(features_test))\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_pred = pd.merge(df_test['id'].astype(int) , df_pred , how = 'outer' , right_index = True , left_index=True)\n",
    "df_pred\n",
    "#評価\n",
    "df_pred.to_csv('XGBsample_submit.csv' , header = False , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29.0\n",
      "1    31.9\n",
      "2    19.0\n",
      "3    28.0\n",
      "4    37.7\n",
      "Name: mpg, dtype: float64\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "             min_child_weight=4, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=300, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "#メーカーと国を合わせたカラム作成\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('データ/train.tsv' , sep = '\\t')\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_train_test = pd.concat([df_train , df_test] , axis =0)\n",
    "#print(df_train_test[df_train_test['horsepower'] == \"?\"])\n",
    "df_train_test['horsepower'] = df_train_test['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train_test.iloc[24 , 4] = df_train_test[df_train_test['displacement'] == 151]['horsepower'].mean()\n",
    "#renault 18\n",
    "df_train_test.iloc[113 , 4] = df_train_test[(98 <= df_train_test['displacement']) & (df_train_test['displacement']<= 102)]['horsepower'].mean()\n",
    "#renault lecar deluxe\n",
    "df_train_test.iloc[145 , 4] = df_train_test[df_train_test['displacement'] == 85]['horsepower'].mean()\n",
    "#ford pinto\n",
    "df_train_test.iloc[175 , 4] = df_train_test[df_train_test['car name'] == 'ford pinto']['horsepower'].mean()\n",
    "#ford maverick\n",
    "df_train_test.loc[70 , 'horsepower'] = df_train_test[df_train_test['car name'] == \"ford maverick\"]['horsepower'].mean()\n",
    "#ford mustang cobra\n",
    "df_train_test.loc[112 , 'horsepower'] = df_train_test[df_train_test['displacement'] == 140]['horsepower'].mean()\n",
    "#pwrカラム作成\n",
    "#df_train_test['pwr'] = df_train_test['weight']/df_train_test['horsepower']\n",
    "#ラベルエンコーディング(train,test結合版)\n",
    "#メーカーカラム作成\n",
    "df_split = df_train_test['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train_test['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train_test['manufacturers name'] = df_train_test['manufacturers name'].replace({\"toyouta\":\"toyota\", \"vw\":\"volkswagen\", \"vokswagen\":\"volkswagen\",  \"chevroelt\":\"chevrolet\" ,  \"chevy\":\"chevrolet\",\"mercury\":\"ford\", \"datsun\":\"nissan\", \"maxda\":\"mazda\",  \"mercedes\":\"mercedes-benz\"})#カラム名の修正\n",
    "#国とメーカーカラムの結合\n",
    "df_train_test['manufacturers_country'] = df_train_test['manufacturers name'] + df_train_test['origin'].astype(str)\n",
    "#エンコーディング\n",
    "df_number = pd.get_dummies(df_train_test , columns = ['manufacturers_country'] , dtype = int)\n",
    "df_train = df_number.iloc[:198 , :]\n",
    "df_test = df_number.iloc[199: , :]\n",
    "\n",
    "#目的変数の分離\n",
    "features_train = df_train.drop([ 'id' , 'car name' , 'mpg' , 'cylinders','weight' , 'manufacturers name'] , axis = 1)\n",
    "target_train = df_train['mpg']\n",
    "print(target_train.head())\n",
    "features_test = df_test.drop([  'id' , 'car name' , 'mpg', 'cylinders' , 'weight' ,'manufacturers name'] , axis = 1)\n",
    "#学習\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor()\n",
    "#K-分割交差検証\n",
    "#交差検証は比較的少ないデータセットを使って学習する場合に、過学習を防ぐ（汎化性能を上げる）ためにする\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 最終的なパラメータ範囲\n",
    "cv_params = {'n_estimators': [100 ,200 ,300],\n",
    "             'learning_rate': [0.01, 0.03, 0.1, 0.3],#学習率(0～1)\n",
    "             'min_child_weight': [2, 4, 6, 8],#決定木の葉の重みの下限\n",
    "             'max_depth': [1, 2, 3, 4],#決定木の最大深度(整数)\n",
    "             'colsample_bytree': [0.2, 0.5, 0.8, 1.0],#説明変数のサンプル抽出比(木)(0~1)\n",
    "             'subsample': [0.2, 0.5, 0.8, 1.0]#各決定木のサンプル抽出比(0~1)、小さいほど保守的になる\n",
    "             }\n",
    "# グリッドサーチのインスタンス作成\n",
    "#引数一覧\n",
    "#estimator: チューニングを行うモデル\n",
    "#param_grid: パラメータ候補パラメータ名: [候補リスト]\n",
    "#scoring: 評価指標(今回はneg_mean_squared_error(RSME))\n",
    "#cv: Cross Validationの分割数(default: 3)\n",
    "#verbose: ログ出力レベル\n",
    "#n_jobs: 同時実行数(-1: コア数で並列実行)\n",
    "#refit: 　trueのとき最良のパラメータで再学習\n",
    "\n",
    " \n",
    "grid_model = GridSearchCV(model, cv_params,cv = cv,\n",
    "                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_model.fit(features_train , target_train)\n",
    "best = grid_model.best_estimator_\n",
    "print(best)\n",
    "df_pred = pd.DataFrame(grid_model.predict(features_test))\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_pred = pd.merge(df_test['id'].astype(int) , df_pred , how = 'outer' , right_index = True , left_index=True)\n",
    "df_pred\n",
    "#評価\n",
    "df_pred.to_csv('XGB_pwr_sample_submit.csv' , header = False , index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29.0\n",
      "1    31.9\n",
      "2    19.0\n",
      "3    28.0\n",
      "4    37.7\n",
      "Name: mpg, dtype: float64\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "             min_child_weight=4, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=300, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "#モデル実装(train,test結合版)#メーカーと国は別\n",
    "#gridsearch版\n",
    "\n",
    "#kfoldを入れ、pwrカラムを追加しない時が一番高かった。\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('データ/train.tsv' , sep = '\\t')\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_train_test = pd.concat([df_train , df_test] , axis =0)\n",
    "#print(df_train_test[df_train_test['horsepower'] == \"?\"])\n",
    "df_train_test['horsepower'] = df_train_test['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train_test.iloc[24 , 4] = df_train_test[df_train_test['displacement'] == 151]['horsepower'].mean()\n",
    "#renault 18\n",
    "df_train_test.iloc[113 , 4] = df_train_test[(98 <= df_train_test['displacement']) & (df_train_test['displacement']<= 102)]['horsepower'].mean()\n",
    "#renault lecar deluxe\n",
    "df_train_test.iloc[145 , 4] = df_train_test[df_train_test['displacement'] == 85]['horsepower'].mean()\n",
    "#ford pinto\n",
    "df_train_test.iloc[175 , 4] = df_train_test[df_train_test['car name'] == 'ford pinto']['horsepower'].mean()\n",
    "#ford maverick\n",
    "df_train_test.loc[70 , 'horsepower'] = df_train_test[df_train_test['car name'] == \"ford maverick\"]['horsepower'].mean()\n",
    "#ford mustang cobra\n",
    "df_train_test.loc[112 , 'horsepower'] = df_train_test[df_train_test['displacement'] == 140]['horsepower'].mean()\n",
    "#pwrカラム作成\n",
    "#df_train_test['pwr'] = df_train_test['weight']/df_train_test['horsepower']\n",
    "#ラベルエンコーディング(train,test結合版)\n",
    "#メーカーカラム作成\n",
    "df_split = df_train_test['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train_test['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train_test['manufacturers name'] = df_train_test['manufacturers name'].replace({\"toyouta\":\"toyota\", \"vw\":\"volkswagen\", \"vokswagen\":\"volkswagen\",  \"chevroelt\":\"chevrolet\" ,  \"chevy\":\"chevrolet\",\"mercury\":\"ford\", \"datsun\":\"nissan\", \"maxda\":\"mazda\",  \"mercedes\":\"mercedes-benz\"})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "df_number = pd.get_dummies(df_train_test , columns = ['origin' , 'manufacturers name'] , dtype = int)\n",
    "df_train = df_number.iloc[:198 , :]\n",
    "df_test = df_number.iloc[199: , :]\n",
    "\n",
    "#目的変数の分離\n",
    "features_train = df_train.drop([ 'id' , 'car name' , 'mpg' , 'cylinders','weight' ] , axis = 1)\n",
    "target_train = df_train['mpg']\n",
    "print(target_train.head())\n",
    "features_test = df_test.drop([  'id' , 'car name' , 'mpg', 'cylinders' , 'weight'] , axis = 1)\n",
    "#学習\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor()\n",
    "#K-分割交差検証\n",
    "#交差検証は比較的少ないデータセットを使って学習する場合に、過学習を防ぐ（汎化性能を上げる）ためにする\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 最終的なパラメータ範囲\n",
    "cv_params = {'n_estimators': [200,300,400],\n",
    "             'learning_rate': [0.01, 0.03, 0.1, 0.3],#学習率(0～1)\n",
    "             'min_child_weight': [2, 4, 6, 8],#決定木の葉の重みの下限\n",
    "             'max_depth': [1, 2, 3, 4],#決定木の最大深度(整数)\n",
    "             'colsample_bytree': [0.2, 0.5, 0.8, 1.0],#説明変数のサンプル抽出比(木)(0~1)\n",
    "             'subsample': [0.2, 0.5, 0.8, 1.0]#各決定木のサンプル抽出比(0~1)、小さいほど保守的になる\n",
    "             }\n",
    "# グリッドサーチのインスタンス作成\n",
    "#引数一覧\n",
    "#estimator: チューニングを行うモデル\n",
    "#param_grid: パラメータ候補パラメータ名: [候補リスト]\n",
    "#scoring: 評価指標(今回はneg_mean_squared_error(RSME))\n",
    "#cv: Cross Validationの分割数(default: 3)\n",
    "#verbose: ログ出力レベル\n",
    "#n_jobs: 同時実行数(-1: コア数で並列実行)\n",
    "#refit: 　trueのとき最良のパラメータで再学習\n",
    "\n",
    " \n",
    "grid_model = GridSearchCV(model, cv_params,cv = cv,\n",
    "                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_model.fit(features_train , target_train)\n",
    "best = grid_model.best_estimator_\n",
    "print(best)\n",
    "df_pred = pd.DataFrame(grid_model.predict(features_test))\n",
    "df_test = pd.read_csv('データ/test.tsv' , sep = '\\t')\n",
    "df_pred = pd.merge(df_test['id'].astype(int) , df_pred , how = 'outer' , right_index = True , left_index=True)\n",
    "df_pred\n",
    "#評価\n",
    "df_pred.to_csv('XGBsample_submit.csv' , header = False , index = False)\n",
    "#29.31(300)pwrあり\n",
    "#29.37\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#目的変数の分離\n",
    "features_train = df_train.drop(['weight' , 'acceleration' , 'mpg' ] , axis = 1)\n",
    "target_train = df_train['mpg']\n",
    "features_test = df_test.drop(['weight' , 'acceleration'] , axis = 1)\n",
    "#学習\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor(n_estimators = 100)\n",
    "model.fit(features_train , target_train)\n",
    "#予測\n",
    "df_pred = pd.DataFrame(model.predict(features_test))\n",
    "df_test = pd.read_csv('../test.tsv' , sep = '\\t')\n",
    "df_pred = pd.merge(df_test['id'].astype(int) , df_pred , how = 'outer' , right_index = True , left_index=True)\n",
    "df_pred\n",
    "#評価\n",
    "df_pred.to_csv('XGBsample_submit.csv' , header = False , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV()\n",
    "# 最終的なパラメータ範囲\n",
    "cv_params = {'n_estimators': [20,40,60,80,100],\n",
    "             'learning_rate': [0.01, 0.03, 0.1, 0.3],#学習率(0～1)\n",
    "             'min_child_weight': [2, 4, 6, 8],#決定木の葉の重みの下限\n",
    "             'max_depth': [1, 2, 3, 4],#決定木の最大深度(整数)\n",
    "             'colsample_bytree': [0.2, 0.5, 0.8, 1.0],#説明変数のサンプル抽出比(木)(0~1)\n",
    "             'subsample': [0.2, 0.5, 0.8, 1.0]#各決定木のサンプル抽出比(0~1)、小さいほど保守的になる\n",
    "             }\n",
    "# グリッドサーチのインスタンス作成\n",
    "#引数一覧\n",
    "#estimator: チューニングを行うモデル\n",
    "#param_grid: パラメータ候補パラメータ名: [候補リスト]\n",
    "#scoring: 評価指標(今回はneg_mean_squared_error(RSME))\n",
    "#cv: Cross Validationの分割数(default: 3)\n",
    "#verbose: ログ出力レベル\n",
    "#n_jobs: 同時実行数(-1: コア数で並列実行)\n",
    "#refit: 　trueのとき最良のパラメータで再学習\n",
    "grid_model = GridSearchCV(model, cv_params,cv = 5,\n",
    "                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_model.fit(features_train , target_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
