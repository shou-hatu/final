{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各ライブラリのimport\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損値補完候補探索\n",
    "#car_name = ['amc concord dl' , 'renault 18i' , 'ford pinto' , 'ford maverick' , 'ford mustang cobra']\n",
    "#df_train_test[df_train_test['car name'] == car_name[3]]\n",
    "#ford pintoは5台存在\n",
    "#ford maverickは3台存在\n",
    "#displacement =[151 , 100 , 85 , 140]\n",
    "#df_train_test[df_train_test['displacement'] == displacement[0]]\n",
    "#df_train_test[(98 <= df_train_test['displacement']) & (df_train_test['displacement']<= 102)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
      "       'acceleration', 'model year', 'origin', 'car name',\n",
      "       'manufacturers name', 'dodge', 'vw', 'toyota', 'pontiac', 'chevrolet',\n",
      "       'oldsmobile', 'bmw', 'mercedes-benz', 'datsun', 'amc', 'renault',\n",
      "       'peugeot', 'ford', 'mercury', 'subaru', 'honda', 'volkswagen', 'saab',\n",
      "       'mazda', 'plymouth', 'opel', 'chevy', 'capri', 'fiat', 'hi', 'audi',\n",
      "       'buick', 'vokswagen', 'volvo', 'triumph', 'America', 'Europe', 'Japan'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#データ読み込みから標準化まで(train,test結合版)\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('../train.tsv' , sep = '\\t')\n",
    "df_test = pd.read_csv('../test.tsv' , sep = '\\t')\n",
    "df_train_test = pd.concat([df_train , df_test] , axis =0)\n",
    "#print(df_train_test[df_train_test['horsepower'] == \"?\"])\n",
    "df_train_test['horsepower'] = df_train_test['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train_test[df_train_test['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train_test.iloc[24 , 4] = df_train_test[df_train_test['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train_test.iloc[113 , 4] = df_train_test[(98 <= df_train_test['displacement']) & (df_train_test['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train_test.iloc[145 , 4] = df_train_test[df_train_test['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train_test.iloc[175 , 4] = df_train_test[df_train_test['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "#ford maverick\n",
    "df_train_test.loc[70 , 'horsepower'] = df_train_test[df_train_test['car name'] == \"ford maverick\"]['horsepower'].mean(numeric_only=True)\n",
    "#ford mustang cobra\n",
    "df_train_test.loc[112 , 'horsepower'] = df_train_test[df_train_test['displacement'] == 140]['horsepower'].mean(numeric_only=True)\n",
    "#補完確認\n",
    "df_train_test[df_train_test['horsepower'].isnull()]\n",
    "#ラベルエンコーディング(train,test結合版)\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train_test['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train_test['manufacturers name'] = df_train_test['manufacturers name'].replace({'chevroelt':'chevrolet' , 'toyouta':'toyota'})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohencoder = OneHotEncoder(sparse=False , dtype=int)\n",
    "#メーカーごとにone-hot\n",
    "df_manufacturers_number= pd.DataFrame(ohencoder.fit_transform(df_train_test['manufacturers name'].values.reshape(-1,1)) , columns = df_train_test['manufacturers name'].unique()).astype(int)\n",
    "#国ごとにone-hot\n",
    "df_origin_number= pd.DataFrame(ohencoder.fit_transform(df_train_test['origin'].values.reshape(-1,1)) , columns = ['America' , 'Europe' , 'Japan']).astype(int)\n",
    "#ダミー変数に変換したdfの結合\n",
    "df_number = pd.merge(df_manufacturers_number , df_origin_number , how = 'outer' , right_index = True , left_index=True)\n",
    "#df_numberとdf_train_testの結合\n",
    "#右側のデータフレームのインデックスをキーとして、左側のデータフレームを結合する(right_index=True)\n",
    "#右側のデータフレームのインデックスをキーとして、左側のデータフレームを結合する(right_index=True)\n",
    "#left_indexとright_indexの両方をTrueにしないとエラーになってしまった\n",
    "df_train = pd.merge(df_train_test , df_number , how = 'outer' , right_index = True , left_index=True)\n",
    "df_train = df_train.dropna(how = 'any')\n",
    "print(df_train.columns)\n",
    "df_test = pd.merge(df_train_test , df_number , how = 'outer' , right_index = True , left_index=True)\n",
    "df_test = df_test.dropna(how = 'any')\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_train_drop = df_train.drop(['id' , 'car name' , 'manufacturers name' , 'origin'] , axis = 1)\n",
    "df_test_drop = df_test.drop(['id' , 'car name' , 'manufacturers name' , 'origin'] , axis = 1)\n",
    "scaler.fit(df_train_drop)\n",
    "df_train_std= pd.DataFrame(scaler.transform(df_train_drop), columns = df_train_drop.columns)\n",
    "df_test_std = pd.DataFrame(scaler.transform(df_train_drop), columns = df_train_drop.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199 entries, 0 to 198\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           199 non-null    float64\n",
      " 1   cylinders     199 non-null    int64  \n",
      " 2   displacement  199 non-null    float64\n",
      " 3   horsepower    199 non-null    float64\n",
      " 4   weight        199 non-null    float64\n",
      " 5   acceleration  199 non-null    float64\n",
      " 6   model year    199 non-null    int64  \n",
      "dtypes: float64(5), int64(2)\n",
      "memory usage: 11.0 KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#読み込みから標準化まで(trainのみ)\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('../train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train.iloc[24 , 4] = df_train[df_train['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train.iloc[113 , 4] = df_train[(98 <= df_train['displacement']) & (df_train['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train.iloc[145 , 4] = df_train[df_train['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train.iloc[175 , 4] = df_train[df_train['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train['manufacturers name'] = df_train['manufacturers name'].replace({'chevroelt':'chevrolet' , 'toyouta':'toyota'})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohencoder = OneHotEncoder(sparse=False , dtype=int)\n",
    "#メーカーごとにone-hot\n",
    "df_train_manufacturers_number= pd.DataFrame(ohencoder.fit_transform(df_train['manufacturers name'].values.reshape(-1,1)) , columns = df_train['manufacturers name'].unique()).astype(int)\n",
    "#国ごとにone-hot\n",
    "df_train_origin_number= pd.DataFrame(ohencoder.fit_transform(df_train['origin'].values.reshape(-1,1)) , columns = ['America' , 'Europe' , 'Japan']).astype(int)\n",
    "#ダミー変数に変換したdfの結合\n",
    "df_train_number = pd.merge(df_train_manufacturers_number , df_train_origin_number , how = 'outer' , right_index = True , left_index=True)\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_train_drop = df_train.drop(['id' , 'manufacturers name' , 'car name', 'origin'] , axis = 1)\n",
    "print(df_train_drop.info())\n",
    "df_train_std = pd.DataFrame(scaler.fit_transform(df_train_drop), columns = df_train_drop.columns)\n",
    "\n",
    "df_train = pd.merge(df_train_std , df_train_number , how = 'outer' , right_index = True , left_index=True)\n",
    "df_train = df_train.dropna(how = 'any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>amc concord dl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>234</td>\n",
       "      <td>34.5</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>renault 18i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>288</td>\n",
       "      <td>40.9</td>\n",
       "      <td>4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>renault lecar deluxe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>349</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>ford pinto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "24    48  23.0          4         151.0         NaN  3035.0          20.5   \n",
       "113  234  34.5          4         100.0         NaN  2320.0          15.8   \n",
       "145  288  40.9          4          85.0         NaN  1835.0          17.3   \n",
       "175  349  25.0          4          98.0         NaN  2046.0          19.0   \n",
       "\n",
       "     model year  origin              car name  \n",
       "24           82       1        amc concord dl  \n",
       "113          81       2           renault 18i  \n",
       "145          80       2  renault lecar deluxe  \n",
       "175          71       1            ford pinto  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データ読み込み\n",
    "df_train = pd.read_csv('../train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
      "       'acceleration', 'model year', 'origin', 'car name',\n",
      "       'manufacturers name', 'dodge', 'vw', 'toyota', 'pontiac', 'chevrolet',\n",
      "       'oldsmobile', 'bmw', 'mercedes-benz', 'datsun', 'amc', 'renault',\n",
      "       'peugeot', 'ford', 'mercury', 'subaru', 'honda', 'volkswagen', 'saab',\n",
      "       'mazda', 'plymouth', 'opel', 'chevy', 'capri', 'fiat', 'hi', 'audi',\n",
      "       'buick', 'vokswagen', 'volvo', 'triumph', 'America', 'Europe', 'Japan'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6003598885538883"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainデータを用いて実装\n",
    "#ランダムフォレスト(weight,cylinder(排気量と相関が強いカラム)抜きが一番精度高かった)\n",
    "#読み込みから標準化まで(trainのみ)\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('../train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train.iloc[24 , 4] = df_train[df_train['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train.iloc[113 , 4] = df_train[(98 <= df_train['displacement']) & (df_train['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train.iloc[145 , 4] = df_train[df_train['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train.iloc[175 , 4] = df_train[df_train['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train['manufacturers name'] = df_train['manufacturers name'].replace({'chevroelt':'chevrolet' , 'toyouta':'toyota'})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohencoder = OneHotEncoder(sparse=False , dtype=int)\n",
    "#メーカーごとにone-hot\n",
    "df_train_manufacturers_number= pd.DataFrame(ohencoder.fit_transform(df_train['manufacturers name'].values.reshape(-1,1)) , columns = df_train['manufacturers name'].unique()).astype(int)\n",
    "#国ごとにone-hot\n",
    "df_train_origin_number= pd.DataFrame(ohencoder.fit_transform(df_train['origin'].values.reshape(-1,1)) , columns = ['America' , 'Europe' , 'Japan']).astype(int)\n",
    "#ダミー変数に変換したdfの結合\n",
    "df_train_number = pd.merge(df_train_manufacturers_number , df_train_origin_number , how = 'outer' , right_index = True , left_index=True)\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train , df_train_number , how = 'outer' , right_index = True , left_index=True)\n",
    "df_train = df_train.dropna(how = 'any')\n",
    "print(df_train.columns)\n",
    "\n",
    "#目的変数の分離\n",
    "features = df_train.drop(['mpg' , 'manufacturers name' , 'car name' , 'id'  ,'cylinders' , 'origin' , 'weight'] , axis = 1)\n",
    "target = df_train['mpg']\n",
    "#データ分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train , features_test , target_train , target_test = train_test_split(features , target , test_size = 0.3 , random_state = 0)\n",
    "\n",
    "#学習\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100 , random_state = 0)\n",
    "model.fit(features_train , target_train)\n",
    "#予測\n",
    "pred = model.predict(features_test)\n",
    "#評価\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "sqrt(mean_squared_error(target_test, pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4029484468959302"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重回帰分析\n",
    "#読み込みから標準化まで(trainのみ)\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('../train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train.iloc[24 , 4] = df_train[df_train['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train.iloc[113 , 4] = df_train[(98 <= df_train['displacement']) & (df_train['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train.iloc[145 , 4] = df_train[df_train['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train.iloc[175 , 4] = df_train[df_train['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train['manufacturers name'] = df_train['manufacturers name'].replace({'chevroelt':'chevrolet' , 'toyouta':'toyota'})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohencoder = OneHotEncoder(sparse=False , dtype=int)\n",
    "#メーカーごとにone-hot\n",
    "df_train_manufacturers_number= pd.DataFrame(ohencoder.fit_transform(df_train['manufacturers name'].values.reshape(-1,1)) , columns = df_train['manufacturers name'].unique()).astype(int)\n",
    "#国ごとにone-hot\n",
    "df_train_origin_number= pd.DataFrame(ohencoder.fit_transform(df_train['origin'].values.reshape(-1,1)) , columns = ['America' , 'Europe' , 'Japan']).astype(int)\n",
    "#ダミー変数に変換したdfの結合\n",
    "df_train_number = pd.merge(df_train_manufacturers_number , df_train_origin_number , how = 'outer' , right_index = True , left_index=True)\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_train_drop = df_train.drop(['id' , 'manufacturers name' , 'car name', 'origin'] , axis = 1)\n",
    "df_train_std = pd.DataFrame(scaler.fit_transform(df_train_drop), columns = df_train_drop.columns)\n",
    "\n",
    "df_train = pd.merge(df_train_std , df_train_number , how = 'outer' , right_index = True , left_index=True)\n",
    "df_train = df_train.dropna(how = 'any')\n",
    "\n",
    "#特徴量の分割\n",
    "features = df_train_std.drop(['mpg','cylinders' , 'weight'] , axis = 1)\n",
    "target = df_train_std['mpg']\n",
    "#データ分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train , features_test , target_train , target_test = train_test_split(features , target , test_size = 0.25 , random_state = 0)\n",
    "#ライブラリのimport\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "#学習と評価\n",
    "model.fit(features_train , target_train)\n",
    "pred = model.predict(features_test)\n",
    "#評価\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "sqrt(mean_squared_error(target_test, pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニューラルネットワーク\n",
    "#目的変数の分離\n",
    "features = df_train.drop(['id' , 'mpg' , 'horsepower' ,'weight' , 'car name' , 'acceleration', 'origin'] , axis = 1)\n",
    "target = df_train['mpg']\n",
    "#データ分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train , features_test , target_train , traget_test = train_test_split(features , target , test_size = 0.3 , random_state = 0)\n",
    "\n",
    "#学習と評価\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "model = MLPRegressor(hidden_layer_sizes =100 , max_iter=200)\n",
    "model.fit(features_train , target_train)\n",
    "pred = model.predict(features_test)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(target_test , pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have at least 1 validation dataset for early stopping.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\u001b[39;00m\n\u001b[0;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1090\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m (\n\u001b[0;32m   1082\u001b[0m     model,\n\u001b[0;32m   1083\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1089\u001b[0m )\n\u001b[1;32m-> 1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\callback.py:241\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    239\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\callback.py:241\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    239\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\shouh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\callback.py:426\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[1;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[0;32m    424\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evals_log\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# Get data name\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n",
      "\u001b[1;31mValueError\u001b[0m: Must have at least 1 validation dataset for early stopping."
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "#読み込みから標準化まで(trainのみ)\n",
    "#欠損値補完と標準化の間にエンコーディングの操作を入れるとエラーが出てしまったため改善しよう\n",
    "#データ読み込み\n",
    "df_train = pd.read_csv('../train.tsv' , sep = '\\t')\n",
    "df_train['horsepower'] = df_train['horsepower'].replace({'?':np.nan}).astype(float)\n",
    "df_train[df_train['horsepower'].isnull()]#欠損値を含むカラムの抽出\n",
    "\n",
    "#欠損値補完\n",
    "#amc concord dl\n",
    "df_train.iloc[24 , 4] = df_train[df_train['displacement'] == 151]['horsepower'].mean(numeric_only=True)\n",
    "#renault 18\n",
    "df_train.iloc[113 , 4] = df_train[(98 <= df_train['displacement']) & (df_train['displacement']<= 102)]['horsepower'].mean(numeric_only=True)\n",
    "#renault lecar deluxe\n",
    "df_train.iloc[145 , 4] = df_train[df_train['displacement'] == 85]['horsepower'].mean(numeric_only=True)\n",
    "#ford pinto\n",
    "df_train.iloc[175 , 4] = df_train[df_train['car name'] == 'ford pinto']['horsepower'].mean(numeric_only=True)\n",
    "\n",
    "#メーカーカラム作成\n",
    "df_split = df_train['car name'].str.split(expand = True)#str.split('')で文字列を''で分割(入力しないと空白で分割される)\n",
    "df_train['manufacturers name'] = df_split.iloc[:, 0]\n",
    "df_train['manufacturers name'] = df_train['manufacturers name'].replace({'chevroelt':'chevrolet' , 'toyouta':'toyota'})#カラム名の修正\n",
    "\n",
    "#エンコーディング\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohencoder = OneHotEncoder(sparse=False , dtype=int)\n",
    "#メーカーごとにone-hot\n",
    "df_train_manufacturers_number= pd.DataFrame(ohencoder.fit_transform(df_train['manufacturers name'].values.reshape(-1,1)) , columns = df_train['manufacturers name'].unique()).astype(int)\n",
    "#国ごとにone-hot\n",
    "df_train_origin_number= pd.DataFrame(ohencoder.fit_transform(df_train['origin'].values.reshape(-1,1)) , columns = ['America' , 'Europe' , 'Japan']).astype(int)\n",
    "#ダミー変数に変換したdfの結合\n",
    "df_train_number = pd.merge(df_train_manufacturers_number , df_train_origin_number , how = 'outer' , right_index = True , left_index=True)\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_train_drop = df_train.drop(['id' , 'manufacturers name' , 'car name', 'origin'] , axis = 1)\n",
    "df_train_std = pd.DataFrame(scaler.fit_transform(df_train_drop), columns = df_train_drop.columns)\n",
    "\n",
    "df_train = pd.merge(df_train_std , df_train_number , how = 'outer' , right_index = True , left_index=True)\n",
    "df_train = df_train.dropna(how = 'any')\n",
    "\n",
    "#特徴量の分割\n",
    "features = df_train_std.drop(['mpg','cylinders'] , axis = 1)\n",
    "target = df_train_std['mpg']\n",
    "#データ分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train , features_test , target_train , target_test = train_test_split(features , target , test_size = 0.25 , random_state = 0)\n",
    "#学習と評価\n",
    "from xgboost import XGBRegressor\n",
    "#early_stopping_roundsは学習回数を適切なタイミングで打ち止めるための仕組み\n",
    "model = XGBRegressor(n_estimators = 1000, early_stopping_rounds = 10)\n",
    "model.fit(features_train , target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
